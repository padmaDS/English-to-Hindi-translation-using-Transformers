{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCW2YRJrECx2",
        "outputId": "e1a7a03f-479d-4df5-a378-f1caff877015"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Aug  5 06:02:48 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#Checking if GPU is running or not\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers[sentencepiece] sacrebleu -q"
      ],
      "metadata": {
        "id": "Xkz1CHXuEvN7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import transformers\n",
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
        "from transformers import AdamWeightDecay\n",
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM"
      ],
      "metadata": {
        "id": "OWgMqOvoE3_A"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"Helsinki-NLP/opus-mt-en-hi\""
      ],
      "metadata": {
        "id": "sr_45_0JFDU1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helsinki-NLP/opus-mt-en-hi model\n",
        "source: https://huggingface.co/Helsinki-NLP/opus-mt-en-hi"
      ],
      "metadata": {
        "id": "Y-v2yJHOFaFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Dataset\n",
        "Source: https://huggingface.co/datasets/cfilt/iitb-english-hindi"
      ],
      "metadata": {
        "id": "dOIo3igCFd3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets = load_dataset(\"cfilt/iitb-english-hindi\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xi5GHznFNhQ",
        "outputId": "964700ff-6ac3-4d74-f7ee-a6c4e61e53a4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaMswUUxFYpb",
        "outputId": "472f8f1c-3935-4b69-9cb6-bc2e3997ca81"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 1659083\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 520\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 2507\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train'][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DVh1I4ZF1yQ",
        "outputId": "e944c2cb-4f86-4484-9599-0f083e0d19dd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'translation': {'en': 'Accerciser Accessibility Explorer',\n",
              "  'hi': 'एक्सेर्साइसर पहुंचनीयता अन्वेषक'}}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the data"
      ],
      "metadata": {
        "id": "72oQ6VfJG5pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZbZYJidGJ2h",
        "outputId": "fad241fa-ff46-4187-cc3c-adc30fbc068d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\"Hello, this is a sentence!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbNvKA79G-1j",
        "outputId": "49be1247-dbb6-4f5d-8d9d-806b439b083d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [12110, 2, 90, 23, 19, 8800, 61, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\"Hello, I am padma!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Qack9miHkMS",
        "outputId": "c9f07945-25af-41a8-979a-cc8f7f5d2971"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [12110, 2, 56, 489, 44, 14586, 8038, 61, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer([\"Hello, this is a sentence!\", \"This is another sentence.\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcuEIodoHoel",
        "outputId": "3845feab-004b-4712-d211-4ce6e2f7097f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[12110, 2, 90, 23, 19, 8800, 61, 0], [239, 23, 414, 8800, 3, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with tokenizer.as_target_tokenizer():\n",
        "    print(tokenizer([\"एक्सेर्साइसर पहुंचनीयता अन्वेषक\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pap2eaQmHutK",
        "outputId": "c8dac046-d0d8-48f0-e9cc-3eb79f4ec536"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[26618, 16155, 346, 33383, 0]], 'attention_mask': [[1, 1, 1, 1, 1]]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3635: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_length = 128\n",
        "max_target_length = 128\n",
        "\n",
        "source_lang = \"en\"\n",
        "target_lang = \"hi\"\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [ex[source_lang] for ex in examples[\"translation\"]]\n",
        "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "IhCuNGOjH8G8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_function(raw_datasets[\"train\"][:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuX8_LV-IMVv",
        "outputId": "2c72b695-212c-48ce-ab8e-de0a5d741539"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[3872, 85, 2501, 132, 15441, 36398, 0], [32643, 28541, 36253, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1]], 'labels': [[63, 2025, 18, 16155, 346, 20311, 24, 2279, 679, 0], [26618, 16155, 346, 33383, 0]]}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "tuNyQMD3IQoP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_b_vF2cIXP5",
        "outputId": "ac95768a-4ff4-4cee-e2d8-63fd9b8fc601"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
            "\n",
            "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-hi.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "learning_rate = 2e-5\n",
        "weight_decay = 0.01\n",
        "num_train_epochs = 10"
      ],
      "metadata": {
        "id": "g9lEH1fuK9MB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"
      ],
      "metadata": {
        "id": "M1WGpgWGLHxz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\", pad_to_multiple_of=128)"
      ],
      "metadata": {
        "id": "sVeycQ-0LUKj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets[\"test\"],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "AU0vopvdLViy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets[\"validation\"],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "DeKFoho8Lnw4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_datasets[\"validation\"],\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    collate_fn=generation_data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "09Ft-uP3LsX4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
        "model.compile(optimizer=optimizer)"
      ],
      "metadata": {
        "id": "aJa5HARbLvZw"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_dataset, validation_data=validation_dataset, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCBkBL-HLx3E",
        "outputId": "5662922a-2bbb-42b6-821c-a86d4946c260"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "156/156 [==============================] - 98s 359ms/step - loss: 3.7686 - val_loss: 3.9454\n",
            "Epoch 2/10\n",
            "156/156 [==============================] - 50s 321ms/step - loss: 3.3201 - val_loss: 3.8700\n",
            "Epoch 3/10\n",
            "156/156 [==============================] - 48s 309ms/step - loss: 3.0186 - val_loss: 3.8354\n",
            "Epoch 4/10\n",
            "156/156 [==============================] - 50s 321ms/step - loss: 2.7796 - val_loss: 3.8120\n",
            "Epoch 5/10\n",
            "156/156 [==============================] - 48s 310ms/step - loss: 2.5603 - val_loss: 3.8180\n",
            "Epoch 6/10\n",
            "156/156 [==============================] - 48s 307ms/step - loss: 2.3789 - val_loss: 3.8225\n",
            "Epoch 7/10\n",
            "156/156 [==============================] - 50s 322ms/step - loss: 2.2036 - val_loss: 3.8327\n",
            "Epoch 8/10\n",
            "156/156 [==============================] - 50s 322ms/step - loss: 2.0554 - val_loss: 3.8375\n",
            "Epoch 9/10\n",
            "156/156 [==============================] - 47s 302ms/step - loss: 1.9046 - val_loss: 3.8419\n",
            "Epoch 10/10\n",
            "156/156 [==============================] - 50s 319ms/step - loss: 1.7708 - val_loss: 3.8748\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7df0b076ff10>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"tf_model/\")"
      ],
      "metadata": {
        "id": "L_hEsGI9L0qG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Testing"
      ],
      "metadata": {
        "id": "m2XJbBnqMK9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"tf_model/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvSuGHX7MKSG",
        "outputId": "71922685-741d-405e-99ba-5c467b0d2e08"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
            "\n",
            "All the layers of TFMarianMTModel were initialized from the model checkpoint at tf_model/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text  = \"My name padma, I am a datascientist\"\n",
        "\n",
        "tokenized = tokenizer([input_text], return_tensors='np')\n",
        "out = model.generate(**tokenized, max_length=128)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xsuVQbGMSGf",
        "outputId": "987fac9c-48cd-43bb-c10f-799f2bb6cfd3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[61949   500   179 21183  4807     2   104    38  4977 10972   254     0]], shape=(1, 12), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with tokenizer.as_target_tokenizer():\n",
        "    print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TV7tvzb0Mgpn",
        "outputId": "0aa8f29b-9ad0-45fe-ffd8-15eb20c155f6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "मेरा नाम अल्मा, मैं एक डेटािस्ट हूँ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vftDxCy6MlGF"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}